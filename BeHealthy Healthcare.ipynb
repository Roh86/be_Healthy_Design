{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731f307c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycrf\n",
      "  Downloading pycrf-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pycrf\n",
      "  Building wheel for pycrf (setup.py): started\n",
      "  Building wheel for pycrf (setup.py): finished with status 'done'\n",
      "  Created wheel for pycrf: filename=pycrf-0.0.1-py3-none-any.whl size=1875 sha256=501d379581826b7197f8e6ffb81cbb478804160ca4ad15fa455f9fe28d6e069f\n",
      "  Stored in directory: c:\\users\\rohit_pandey1\\appdata\\local\\pip\\cache\\wheels\\e3\\d2\\c9\\ba15b05ba596e2eafeb83c2903e79d634207367555aae8c7d2\n",
      "Successfully built pycrf\n",
      "Installing collected packages: pycrf\n",
      "Successfully installed pycrf-0.0.1\n",
      "Collecting sklearn-crfsuite\n",
      "  Obtaining dependency information for sklearn-crfsuite from https://files.pythonhosted.org/packages/b2/11/a8370dd6fce65f8f4e74a0adffae72be9db5799d8ed8ddbf84415356a764/sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite)\n",
      "  Obtaining dependency information for python-crfsuite>=0.9.7 from https://files.pythonhosted.org/packages/f3/4c/2aabe6f3c06a6e62fb4d80d0ed224e3d813b7fc5bc7d9aba52e724639268/python_crfsuite-0.9.11-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading python_crfsuite-0.9.11-cp311-cp311-win_amd64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in c:\\users\\rohit_pandey1\\appdata\\local\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (1.3.0)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in c:\\users\\rohit_pandey1\\appdata\\local\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (0.8.10)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\rohit_pandey1\\appdata\\local\\anaconda3\\lib\\site-packages (from sklearn-crfsuite) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\rohit_pandey1\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\rohit_pandey1\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\rohit_pandey1\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rohit_pandey1\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24.0->sklearn-crfsuite) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohit_pandey1\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=2.0->sklearn-crfsuite) (0.4.6)\n",
      "Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading python_crfsuite-0.9.11-cp311-cp311-win_amd64.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.9 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 30.7/301.9 kB 660.6 kB/s eta 0:00:01\n",
      "   ------------ --------------------------- 92.2/301.9 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 256.0/301.9 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  297.0/301.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 301.9/301.9 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.11 sklearn-crfsuite-0.5.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install pycrf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install sklearn-crfsuite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn_crfsuite\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn_crfsuite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pycrf\n",
    "!pip install sklearn-crfsuite\n",
    "\n",
    "import spacy\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "model = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0b8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing \n",
    "def process_file(filename):\n",
    "  input_file = open(filename, 'r')\n",
    "  file_content = input_file.readlines() \n",
    "  input_file.close()\n",
    "\n",
    "  out_lines = [] #To store list of sequences (sentences or labels)\n",
    "\n",
    "  line_content = \"\"\n",
    "\n",
    "  for word in file_content:\n",
    "    word = word.strip() \n",
    "    if word == \"\": # If empty line, add the current sequence to out_lines\n",
    "      out_lines.append(line_content)\n",
    "      line_content = \"\"; # re-initialize\n",
    "    else:\n",
    "      if line_content: #if non-empty, add new word after space\n",
    "        line_content += \" \"+word\n",
    "      else:\n",
    "        line_content = word # first word, no space required\n",
    "\n",
    "  return out_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4984736d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_sent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_sentences \u001b[38;5;241m=\u001b[39m process_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_sent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m process_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_label\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m test_sentences \u001b[38;5;241m=\u001b[39m process_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_sent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mprocess_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_file\u001b[39m(filename):\n\u001b[1;32m----> 3\u001b[0m   input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m   file_content \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39mreadlines() \n\u001b[0;32m      5\u001b[0m   input_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_sent'"
     ]
    }
   ],
   "source": [
    "train_sentences = process_file('train_sent')\n",
    "train_labels = process_file('train_label')\n",
    "test_sentences = process_file('test_sent')\n",
    "test_labels = process_file('test_label')\n",
    "# Print the 5 sentences from the processed dataset\n",
    "for i in range(5):\n",
    "  print(\"Sentence:\", train_sentences[i])\n",
    "  print(\"Labels:\", train_labels[i], \"\\n\\n\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81dce7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# number of sentences\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo. of lines in train_sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_sentences))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo. of lines in test_sentences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_sentences))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# number of lines of labels\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# number of sentences\n",
    "print(\"No. of lines in train_sentences:\", len(train_sentences))\n",
    "print(\"No. of lines in test_sentences:\", len(test_sentences))\n",
    "\n",
    "# number of lines of labels\n",
    "\n",
    "print(\"No. of lines in train_labels:\", len(train_labels))\n",
    "print(\"No. of lines in test_labels:\", len(test_labels))\n",
    "     \n",
    "# Extract those tokens which have NOUN or PROPN\n",
    "\n",
    "concepts = {}\n",
    "\n",
    "for sentences in (train_sentences, test_sentences):\n",
    "  for sentence in sentences:\n",
    "    processed_sentence = model(sentence) # Process each sentence by spacy model\n",
    "    for token in processed_sentence:\n",
    "      if(token.pos_ == 'NOUN' or token.pos_ == 'PROPN'): #check if the token is a noun\n",
    "        concepts[token.text] = concepts.get(token.text, 0) + 1; #increase its frequency if it is noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54a09674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining CFR \n",
    "def getFeaturesForOneWord(sentence, pos, pos_tags):\n",
    "  word = sentence[pos]\n",
    "\n",
    "  #Define 12 features with PoS tag as one of the features\n",
    "  features = [\n",
    "    'word.lower=' + word.lower(), # serves as word id\n",
    "    'word[-3:]=' + word[-3:],     # last three characters\n",
    "    'word[-2:]=' + word[-2:],     # last two characters\n",
    "    'word.isupper=%s' % word.isupper(),  # is the word in all uppercase\n",
    "    'word.isdigit=%s' % word.isdigit(),  # is the word a number\n",
    "    'word.startsWithCapital=%s' % word[0].isupper(), # is the word starting with a capital letter\n",
    "    'word.pos=' + pos_tags[pos]\n",
    "  ]\n",
    "\n",
    "  #Use the previous word also while defining features\n",
    "  if(pos > 0):\n",
    "    prev_word = sentence[pos-1]\n",
    "    features.extend([\n",
    "    'prev_word.lower=' + prev_word.lower(), \n",
    "    'prev_word.isupper=%s' % prev_word.isupper(),\n",
    "    'prev_word.isdigit=%s' % prev_word.isdigit(),\n",
    "    'prev_word.startsWithCapital=%s' % prev_word[0].isupper(),\n",
    "    'prev_word.pos=' + pos_tags[pos-1]\n",
    "  ])\n",
    "  # Mark the begining and the end words of a sentence correctly in the form of features.\n",
    "  else:\n",
    "    features.append('BEG') # feature to track begin of sentence \n",
    "\n",
    "  if(pos == len(sentence)-1):\n",
    "    features.append('END') # feature to track end of sentence\n",
    "\n",
    "  return features\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad20873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to get features for a sentence using the 'getFeaturesForOneWord' function.\n",
    "def getFeaturesForOneSentence(sentence):\n",
    "  \n",
    "  processed_sentence = model(sentence) #spacy is applied to sentence\n",
    "  \n",
    "  pos_tags = [] #correctly identify pos tags\n",
    "  for token in processed_sentence:\n",
    "    pos_tags.append(token.pos_)\n",
    "\n",
    "  sentence_list = sentence.split() # List of words in sentence\n",
    "  \n",
    "  #Correctly calling getFeaturesForOneWord defined above\n",
    "  return [getFeaturesForOneWord(sentence_list, pos, pos_tags) for pos in range(len(sentence_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e7a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the labels for a sentence.\n",
    "def getLabelsInListForOneSentence(labels):\n",
    "  return labels.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9553c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train \u001b[38;5;241m=\u001b[39m [getFeaturesForOneSentence(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m train_sentences]\n\u001b[0;32m      2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m [getFeaturesForOneSentence(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m test_sentences]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = [getFeaturesForOneSentence(sentence) for sentence in train_sentences]\n",
    "X_test = [getFeaturesForOneSentence(sentence) for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fcd397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:37: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:37: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:37: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:37: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Y_train \u001b[38;5;241m=\u001b[39m [getLabelsInListForOneSentence(labels) \u001b[38;5;28;01mfor\u001b[39;00m labels \u001b[38;5;129;01min\u001b[39;00m train_labels]\n\u001b[0;32m      2\u001b[0m Y_test \u001b[38;5;241m=\u001b[39m [getLabelsInListForOneSentence(labels) \u001b[38;5;28;01mfor\u001b[39;00m labels \u001b[38;5;129;01min\u001b[39;00m test_labels]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Build the CRF model.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Calling CRF \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "Y_train = [getLabelsInListForOneSentence(labels) for labels in train_labels]\n",
    "Y_test = [getLabelsInListForOneSentence(labels) for labels in test_labels]\n",
    "# Build the CRF model.\n",
    "\n",
    "# Calling CRF \n",
    "crf = sklearn_crfsuite.CRF(max_iterations=100)\n",
    "\n",
    "# Check that only X_train and Y_train are passed\n",
    "crf.fit(X_train, Y_train)\n",
    "diseases_and_treatments =  {} # dictionary with disease as key an list of treatments as value\n",
    "\n",
    "for i in range(len(Y_pred)): # For each predicted sequence\n",
    "  labels = Y_pred[i]\n",
    "\n",
    "  disease = \"\";\n",
    "  treatment = \"\";\n",
    "  \n",
    "  for j in range(len(labels)): # for each individual label in the sequence\n",
    "    if labels[j] == 'O': # ignore if label is O -- other\n",
    "      continue\n",
    "\n",
    "    if(labels[j] == 'D'): # Label D indicates disease, so add the corresponding word from test sentence to the disease name string\n",
    "      disease += test_sentences[i].split()[j] + \" \"\n",
    "      continue\n",
    "\n",
    "    if(labels[j] == 'T'): # Label T indicates disease, so add the corresponding word from test sentence to the treatment name string\n",
    "      #print(test_sentences[i].split()[j])\n",
    "      treatment += test_sentences[i].split()[j] + \" \"\n",
    "\n",
    "  disease = disease.strip() # to remove extraneous spaces\n",
    "  treatment = treatment.strip()\n",
    "\n",
    "  # add the identified disease and treatment to the dictionary\n",
    "  # if it is a new disease, directly add the value\n",
    "  # if the disease has been seen previously, get the treatment list\n",
    "  # and add current treatment to the list.\n",
    "  if disease is not \"\" and treatment is not \"\":\n",
    "    if disease not in diseases_and_treatments.keys():\n",
    "      diseases_and_treatments[disease] = [treatment]\n",
    "    else:\n",
    "      treatment_list = diseases_and_treatments.get(disease)\n",
    "      treatment_list.append(treatment)\n",
    "      diseases_and_treatments[disease] = treatment_list \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f177d16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diseases_and_treatments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m diseases_identified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(diseases_and_treatments\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      2\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m94\u001b[39m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisease: \u001b[39m\u001b[38;5;124m\"\u001b[39m,diseases_identified[index])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'diseases_and_treatments' is not defined"
     ]
    }
   ],
   "source": [
    "diseases_identified = list(diseases_and_treatments.keys())\n",
    "index = 94 \n",
    "\n",
    "print(\"Disease: \",diseases_identified[index])\n",
    "print(\"Treatment:\", diseases_and_treatments.get(diseases_identified[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb784ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
